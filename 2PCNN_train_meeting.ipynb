{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sys, os, random, gzip\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global flags & parameters\n",
    "# remark: increase the sample size for full deployment, while the training/testing samples should be skipped.\n",
    "SRC_SIG = {'filename':'jet_g_match_qcd_gg.txt.gz', 'skip':60000, 'n':2000}\n",
    "SRC_BKG = {'filename':'jet_q_match_qcd_qq.txt.gz', 'skip':60000, 'n':2000}\n",
    "WEIGHTS_2PCNN = 'wgts_prototype.h5'\n",
    "BATCH_SIZE = 80\n",
    "MAKE_SUMMARY_PLOTS = True\n",
    "EPOCHS = 20\n",
    "ENABLE_EARLY_STOPPING = True\n",
    "MAX_PATIENCE = 10\n",
    "\n",
    "# # constituents format\n",
    "# _index, _type, _pid, _charge, _pt, _eta, _phi, _vx, _vy, _vz = range(10)\n",
    "_pt,_eta,_phi=1,2,3 #myfile use pt in index 1, eta->2,phi->3\n",
    "def deltaPhi(phi1,phi2):\n",
    "    x = phi1-phi2\n",
    "    while x>= np.pi: x -= np.pi*2.\n",
    "    while x< -np.pi: x += np.pi*2.\n",
    "    return x\n",
    "\n",
    "def deltaR(eta1,phi1,eta2,phi2):\n",
    "    return (deltaPhi(phi1,phi2)**2+(eta1-eta2)**2)**0.5\n",
    "\n",
    "def find_main_axis(clist):\n",
    "    def fcn(p):\n",
    "        dir_x,dir_y = np.cos(p[0]),np.sin(p[0])\n",
    "        v = clist[(clist[:,_eta]**2+clist[:,_phi]**2)**0.5>1E-5]\n",
    "        cosang = np.abs(dir_x*v[:,_eta]+dir_y*v[:,_phi])/(v[:,_eta]**2+v[:,_phi]**2)**0.5\n",
    "        proj = (v[:,_pt]*cosang).sum()\n",
    "        return -proj\n",
    "    r = opt.minimize(fcn,[0.])\n",
    "    if r.success: return r.x[0]\n",
    "    else: return None\n",
    "\n",
    "# reading jet data from the gzipped text stream\n",
    "# def parse_jet_data(fin):\n",
    "    \n",
    "#     if '<jet_data>' not in fin.readline().decode():\n",
    "#         print(\">>> ERROR: invalid input\", flush=True)\n",
    "#         sys.exit(0)\n",
    "#     data = {}\n",
    "#     buf = fin.readline().decode().split() # jet kinematics\n",
    "#     data['index'] = int(buf[0])\n",
    "#     data['pt'] = float(buf[1])\n",
    "#     data['eta'] = float(buf[2])\n",
    "#     data['phi'] = float(buf[3])\n",
    "#     data['mass'] = float(buf[4])\n",
    "#     data['deltaeta'] = float(buf[5])\n",
    "#     data['deltaphi'] = float(buf[6])\n",
    "#     data['charge'] = int(buf[7])\n",
    "#     data['ehadovereem'] = float(buf[8])\n",
    "#     data['ncharged'] = int(buf[9])\n",
    "#     data['nneutrals'] = int(buf[10])\n",
    "#     data['tau1'] = float(buf[11])\n",
    "#     data['tau2'] = float(buf[12])\n",
    "#     data['tau3'] = float(buf[13])\n",
    "#     data['tau4'] = float(buf[14])\n",
    "#     data['tau5'] = float(buf[15])\n",
    "#     buf = fin.readline().decode().split() # trimmed/pruned/softdrop P4\n",
    "#     data['pt_trimmed'] = float(buf[0])\n",
    "#     data['eta_trimmed'] = float(buf[1])\n",
    "#     data['phi_trimmed'] = float(buf[2])\n",
    "#     data['mass_trimmed'] = float(buf[3])\n",
    "#     data['pt_pruned'] = float(buf[4])\n",
    "#     data['eta_pruned'] = float(buf[5])\n",
    "#     data['phi_pruned'] = float(buf[6])\n",
    "#     data['mass_pruned'] = float(buf[7])\n",
    "#     data['pt_pruned_sub1'] = float(buf[8])\n",
    "#     data['eta_pruned_sub1'] = float(buf[9])\n",
    "#     data['phi_pruned_sub1'] = float(buf[10])\n",
    "#     data['mass_pruned_sub1'] = float(buf[11])\n",
    "#     data['pt_pruned_sub2'] = float(buf[12])\n",
    "#     data['eta_pruned_sub2'] = float(buf[13])\n",
    "#     data['phi_pruned_sub2'] = float(buf[14])\n",
    "#     data['mass_pruned_sub2'] = float(buf[15])\n",
    "#     data['pt_pruned_sub3'] = float(buf[16])\n",
    "#     data['eta_pruned_sub3'] = float(buf[17])\n",
    "#     data['phi_pruned_sub3'] = float(buf[18])\n",
    "#     data['mass_pruned_sub3'] = float(buf[19])\n",
    "#     data['pt_softdrop'] = float(buf[20])\n",
    "#     data['eta_softdrop'] = float(buf[21])\n",
    "#     data['phi_softdrop'] = float(buf[22])\n",
    "#     data['mass_softdrop'] = float(buf[23])\n",
    "#     data['pt_softdrop_sub1'] = float(buf[24])\n",
    "#     data['eta_softdrop_sub1'] = float(buf[25])\n",
    "#     data['phi_softdrop_sub1'] = float(buf[26])\n",
    "#     data['mass_softdrop_sub1'] = float(buf[27])\n",
    "#     data['pt_softdrop_sub2'] = float(buf[28])\n",
    "#     data['eta_softdrop_sub2'] = float(buf[29])\n",
    "#     data['phi_softdrop_sub2'] = float(buf[30])\n",
    "#     data['mass_softdrop_sub2'] = float(buf[31])\n",
    "#     data['pt_softdrop_sub3'] = float(buf[32])\n",
    "#     data['eta_softdrop_sub3'] = float(buf[33])\n",
    "#     data['phi_softdrop_sub3'] = float(buf[34])\n",
    "#     data['mass_softdrop_sub3'] = float(buf[35])\n",
    "#     buf = fin.readline().decode().split() # subject/constituents counts\n",
    "#     data['nsub_trimmed'] = int(buf[0])\n",
    "#     data['nsub_pruned'] = int(buf[1])\n",
    "#     data['nsub_softdrop'] = int(buf[2])\n",
    "#     data['nconstituents'] = int(buf[3])\n",
    "#     buf = fin.readline().decode().split() # generater info\n",
    "#     data['gen_pid'] = int(buf[0])\n",
    "#     data['gen_charge'] = int(buf[1])\n",
    "#     data['gen_pt'] = float(buf[2])\n",
    "#     data['gen_eta'] = float(buf[3])\n",
    "#     data['gen_phi'] = float(buf[4])\n",
    "#     data['gen_mass'] = float(buf[5])\n",
    "  \n",
    "\n",
    "  \n",
    "#      clist = [] # prepare constituents list\n",
    "#     for i in range(data['nconstituents']):\n",
    "#         var = [float(s) for s in fin.readline().decode().split()] # index, type(0:gen/1:track/2:Ecal/3:Hcal), pid, charge, pt, eta, phi, vx, vy, vz\n",
    "#         # relative to jet pt and direction\n",
    "#         var[_pt ] = var[_pt]/data['pt']\n",
    "#         var[_eta] = var[_eta]-data['eta']\n",
    "#         var[_phi] = deltaPhi(var[_phi],data['phi'])\n",
    "#         clist.append(var)\n",
    "#     clist = np.array(clist)\n",
    "\n",
    "#     buf = fin.readline().decode().split() # Tjet variables, nsub = 1\n",
    "#     data['tjet1_eta1'] = float(buf[0])\n",
    "#     data['tjet1_phi1'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet1_R1'] = float(buf[0])\n",
    "#     data['tjet1_R1_pt1'] = float(buf[1])\n",
    "#     data['tjet1_R1_m1'] = float(buf[2])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet1_R2'] = float(buf[0])\n",
    "#     data['tjet1_R2_pt1'] = float(buf[1])\n",
    "#     data['tjet1_R2_m1'] = float(buf[2])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet1_R3'] = float(buf[0])\n",
    "#     data['tjet1_R3_pt1'] = float(buf[1])\n",
    "#     data['tjet1_R3_m1'] = float(buf[2])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet1_R4'] = float(buf[0])\n",
    "#     data['tjet1_R4_pt1'] = float(buf[1])\n",
    "#     data['tjet1_R4_m1'] = float(buf[2])\n",
    "\n",
    "#     buf = fin.readline().decode().split() # Tjet variables, nsub = 2\n",
    "#     data['tjet2_eta1'] = float(buf[0])\n",
    "#     data['tjet2_phi1'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet2_eta2'] = float(buf[0])\n",
    "#     data['tjet2_phi2'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet2_R1'] = float(buf[0])\n",
    "#     data['tjet2_R1_pt1'] = float(buf[1])\n",
    "#     data['tjet2_R1_pt2'] = float(buf[2])\n",
    "#     data['tjet2_R1_m1'] = float(buf[3])\n",
    "#     data['tjet2_R1_m2'] = float(buf[4])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet2_R2'] = float(buf[0])\n",
    "#     data['tjet2_R2_pt1'] = float(buf[1])\n",
    "#     data['tjet2_R2_pt2'] = float(buf[2])\n",
    "#     data['tjet2_R2_m1'] = float(buf[3])\n",
    "#     data['tjet2_R2_m2'] = float(buf[4])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet2_R3'] = float(buf[0])\n",
    "#     data['tjet2_R3_pt1'] = float(buf[1])\n",
    "#     data['tjet2_R3_pt2'] = float(buf[2])\n",
    "#     data['tjet2_R3_m1'] = float(buf[3])\n",
    "#     data['tjet2_R3_m2'] = float(buf[4])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet2_R4'] = float(buf[0])\n",
    "#     data['tjet2_R4_pt1'] = float(buf[1])\n",
    "#     data['tjet2_R4_pt2'] = float(buf[2])\n",
    "#     data['tjet2_R4_m1'] = float(buf[3])\n",
    "#     data['tjet2_R4_m2'] = float(buf[4])\n",
    "\n",
    "#     buf = fin.readline().decode().split() # Tjet variables, nsub = 3\n",
    "#     data['tjet3_eta1'] = float(buf[0])\n",
    "#     data['tjet3_phi1'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_eta2'] = float(buf[0])\n",
    "#     data['tjet3_phi2'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_eta3'] = float(buf[0])\n",
    "#     data['tjet3_phi3'] = float(buf[1])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_R1'] = float(buf[0])\n",
    "#     data['tjet3_R1_pt1'] = float(buf[1])\n",
    "#     data['tjet3_R1_pt2'] = float(buf[2])\n",
    "#     data['tjet3_R1_pt3'] = float(buf[3])\n",
    "#     data['tjet3_R1_m1'] = float(buf[4])\n",
    "#     data['tjet3_R1_m2'] = float(buf[5])\n",
    "#     data['tjet3_R1_m3'] = float(buf[6])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_R2'] = float(buf[0])\n",
    "#     data['tjet3_R2_pt1'] = float(buf[1])\n",
    "#     data['tjet3_R2_pt2'] = float(buf[2])\n",
    "#     data['tjet3_R2_pt3'] = float(buf[3])\n",
    "#     data['tjet3_R2_m1'] = float(buf[4])\n",
    "#     data['tjet3_R2_m2'] = float(buf[5])\n",
    "#     data['tjet3_R2_m3'] = float(buf[6])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_R3'] = float(buf[0])\n",
    "#     data['tjet3_R3_pt1'] = float(buf[1])\n",
    "#     data['tjet3_R3_pt2'] = float(buf[2])\n",
    "#     data['tjet3_R3_pt3'] = float(buf[3])\n",
    "#     data['tjet3_R3_m1'] = float(buf[4])\n",
    "#     data['tjet3_R3_m2'] = float(buf[5])\n",
    "#     data['tjet3_R3_m3'] = float(buf[6])\n",
    "#     buf = fin.readline().decode().split()\n",
    "#     data['tjet3_R4'] = float(buf[0])\n",
    "#     data['tjet3_R4_pt1'] = float(buf[1])\n",
    "#     data['tjet3_R4_pt2'] = float(buf[2])\n",
    "#     data['tjet3_R4_pt3'] = float(buf[3])\n",
    "#     data['tjet3_R4_m1'] = float(buf[4])\n",
    "#     data['tjet3_R4_m2'] = float(buf[5])\n",
    "#     data['tjet3_R4_m3'] = float(buf[6])\n",
    "\n",
    "#     if '</jet_data>' not in fin.readline().decode():\n",
    "#         print(\">>> ERROR: invalid input\", flush=True)\n",
    "#         sys.exit(0)\n",
    "    \n",
    "#     # Apply rotation\n",
    "#     dir = find_main_axis(clist)\n",
    "#     if dir!=None:\n",
    "#         dir = -dir # rotation everything to x-axis\n",
    "#         clist[:,_eta], clist[:,_phi] = np.cos(dir)*clist[:,_eta]-np.sin(dir)*clist[:,_phi], np.sin(dir)*clist[:,_eta]+np.cos(dir)*clist[:,_phi]\n",
    "\n",
    "#     return data, clist\n",
    "\n",
    "# skip 1 set of jet data from the gzipped text stream\n",
    "def skip_jet_data(fin):\n",
    "    if '<jet_data>' not in fin.readline().decode():\n",
    "        print(\">>> ERROR: invalid input\", flush=True)\n",
    "        sys.exit(0)\n",
    "    while '</jet_data>' not in fin.readline().decode():\n",
    "        pass\n",
    "\n",
    "def prepare_sample(filename, size, skip = 0):\n",
    "    fin = gzip.open(filename)\n",
    "    print('Loading from',filename, flush=True)\n",
    "    \n",
    "    if skip>0:\n",
    "        print('Skip first',skip,'jets', flush=True)\n",
    "        for i in range(skip):\n",
    "            skip_jet_data(fin)\n",
    "    \n",
    "    slist = []\n",
    "    while(len(slist)<size):\n",
    "        d, c = parse_jet_data(fin)\n",
    "        if len(c)<4: continue # drop those jets with low # of constituents\n",
    "        \n",
    "        # inject regular jet data for convenience\n",
    "        supply = np.array([d['pt']/1E3, # in unit of TeV\n",
    "                           d['eta'],\n",
    "                           d['phi']],dtype=K.floatx())\n",
    "        \n",
    "        c_ext = [] # expend the 2-particle correlations\n",
    "        for i,j in [(i,j) for i in range(len(c)) for j in range(i+1,len(c))]:\n",
    "            \n",
    "            # only (pt,eta,phi)*2\n",
    "            c_ext.append([c[i,_pt],c[i,_eta],c[i,_phi],\n",
    "                          c[j,_pt],c[j,_eta],c[j,_phi]])\n",
    "\n",
    "        slist.append((np.array(c_ext,dtype=K.floatx()),supply)) # tuple of 2pc array, supply array\n",
    "        if len(slist) % 1000==0: print(len(slist),'/',size,'jets loaded', flush=True)\n",
    "    fin.close()\n",
    "    return slist\n",
    "\n",
    "# custom 2PC layer\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, filters, toppooling_dim = 1, hidden_dim = None, **kwargs):\n",
    "        self.filters = filters\n",
    "        self.toppooling_dim = toppooling_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.hidden_dim!=None:\n",
    "            self.w1 = self.add_weight(name='w1', shape=(self.filters, input_shape[-1], self.hidden_dim), initializer='normal', trainable=True)\n",
    "            self.b1 = self.add_weight(name='b1', shape=(self.filters, self.hidden_dim), initializer='normal', trainable=True)\n",
    "            self.w2 = self.add_weight(name='w2', shape=(self.filters, self.hidden_dim, 1), initializer='normal', trainable=True)\n",
    "            self.b2 = self.add_weight(name='b2', shape=(self.filters, 1), initializer='normal', trainable=True)\n",
    "        else:\n",
    "            self.w1 = self.add_weight(name='w1', shape=(self.filters, input_shape[-1], 1), initializer='normal', trainable=True)\n",
    "            self.b1 = self.add_weight(name='b1', shape=(self.filters, 1), initializer='normal', trainable=True)\n",
    "        \n",
    "        super(MyLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        def operation_over_sample(sample):\n",
    "            buffer = []\n",
    "            for idx in range(self.filters):\n",
    "                oper = K.dot(sample,self.w1[idx])+self.b1[idx] # sum w*x+b\n",
    "                oper = K.relu(oper) # activation\n",
    "                \n",
    "                if self.hidden_dim!=None: # insert hidden dense layer\n",
    "                    oper = K.dot(oper,self.w2[idx])+self.b2[idx] # sum w*x+b\n",
    "                    oper = K.relu(oper) # activation\n",
    "                \n",
    "                # since the first input is pt, which is always positive => convert it as a mask\n",
    "                mask = K.expand_dims(K.greater(sample[:,0],0.),axis=1)\n",
    "                mask = K.cast(mask,K.floatx())\n",
    "                oper *= mask # apply masking\n",
    "                \n",
    "                top_indices = tf.nn.top_k(oper[:,0], k=self.toppooling_dim, sorted=True).indices\n",
    "                top = K.gather(oper,top_indices) # top-k pooling\n",
    "                \n",
    "                buffer.append(top)\n",
    "            return K.concatenate(buffer)\n",
    "        return K.map_fn(operation_over_sample,x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.toppooling_dim, self.filters) # output dimension = n_samples x toppooling_dim x filters\n",
    "def combine2(a):\n",
    "    return a*(a-1)/2\n",
    "def txttogz(myev): ## txt -> .txt.gz\n",
    "    import gzip\n",
    "    \n",
    "    import shutil\n",
    "    with open(myev, 'rb') as f_in, gzip.open(myev+'.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "def parse_jet_data(fin):\n",
    "    \n",
    "#     if '<jet_data>' not in fin.readline().decode():\n",
    "#         print(\">>> ERROR: invalid input\", flush=True)\n",
    "#         sys.exit(0)\n",
    "    data = {}\n",
    "    fin.readline().decode().split()\n",
    "    buf = fin.readline().decode().split() # jet kinematics\n",
    "    data['pt'] = float(buf[1])\n",
    "    data['eta'] = float(buf[2])\n",
    "    data['phi'] = float(buf[3])\n",
    "    data['nconstituents'] = int(buf[4])\n",
    "    fin.readline().decode().split()\n",
    "    clist = [] # prepare constituents list\n",
    "    for i in range(data['nconstituents']):\n",
    "        var = [float(s) for s in fin.readline().decode().split()] # index, type(0:gen/1:track/2:Ecal/3:Hcal), pid, charge, pt, eta, phi, vx, vy, vz\n",
    "        # relative to jet pt and direction\n",
    "        _pt,_eta,_phi=1,2,3 #myfile use pt in index 1, eta->2,phi->3\n",
    "        var[_pt ] = var[_pt]/data['pt']\n",
    "        var[_eta] = var[_eta]-data['eta']\n",
    "        var[_phi] = deltaPhi(var[_phi],data['phi'])\n",
    "        clist.append(var)\n",
    "    clist = np.array(clist)\n",
    "    dir = find_main_axis(clist)\n",
    "    if dir!=None:\n",
    "        dir = -dir # rotation everything to x-axis\n",
    "        clist[:,_eta], clist[:,_phi] = np.cos(dir)*clist[:,_eta]-np.sin(dir)*clist[:,_phi], np.sin(dir)*clist[:,_eta]+np.cos(dir)*clist[:,_phi]\n",
    "\n",
    "\n",
    "    return data, clist\n",
    "def prepare_sample2(filename, size, skip = 0):\n",
    "    fin = gzip.open(filename)\n",
    "    print('Loading from',filename, flush=True)\n",
    "    \n",
    "#     if skip>0:\n",
    "#         print('Skip first',skip,'jets', flush=True)\n",
    "#         for i in range(skip):\n",
    "#             skip_jet_data(fin)\n",
    "    \n",
    "    slist = []\n",
    "    while(len(slist)<size):\n",
    "        d, c = parse_jet_data(fin)\n",
    "        if len(c)<4: continue # drop those jets with low # of constituents\n",
    "        \n",
    "        # inject regular jet data for convenience\n",
    "        supply = np.array([d['pt']/1E3, # in unit of TeV\n",
    "                           d['eta'],\n",
    "                           d['phi']],dtype=K.floatx())\n",
    "        \n",
    "        c_ext = [] # expend the 2-particle correlations\n",
    "        for i,j in [(i,j) for i in range(len(c)) for j in range(i+1,len(c))]:\n",
    "            \n",
    "            # only (pt,eta,phi)*2\n",
    "            c_ext.append([c[i,_pt],c[i,_eta],c[i,_phi],\n",
    "                          c[j,_pt],c[j,_eta],c[j,_phi]])\n",
    "\n",
    "        slist.append((np.array(c_ext,dtype=K.floatx()),supply)) # tuple of 2pc array, supply array\n",
    "        if len(slist) % 1000==0: print(len(slist),'/',size,'jets loaded', flush=True)\n",
    "    fin.close()\n",
    "    return slist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data prepare myself\n",
    "myev='./myevents'\n",
    "qq,gg='qq','gg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you need it only for first times you run this program\n",
    "txttogz(myev+qq+'.txt') ##txt -> gz\n",
    "txttogz(myev+gg+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist_sig=prepare_sample2(myev+gg+'.txt.gz', 500, skip = 0)\n",
    "slist_bkg=prepare_sample2(myev+qq+'.txt.gz', 500, skip = 0)\n",
    "sample = [[*s,np.array([1.,0.],dtype=K.floatx())] for s in slist_sig]+[[*s,np.array([0.,1.],dtype=K.floatx())] for s in slist_bkg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(sample) # shuffle the sample before training/testing\n",
    "\n",
    "# split training (2/3) and testing (1/3) samples\n",
    "n_training = len(sample)*2//3\n",
    "n_testing = len(sample)-n_training\n",
    "s_train = sample[:n_training]\n",
    "s_test = sample[n_training:]\n",
    "\n",
    "# custom layer for 2-particle correlation\n",
    "x_input = Input(shape=(None,sample[0][0].shape[-1]))\n",
    "x_seq = MyLayer(filters=64, toppooling_dim=4, hidden_dim=2*sample[0][0].shape[-1])(x_input)\n",
    "x_seq = Flatten()(x_seq)\n",
    "\n",
    "# dense layer for supplementary jet data\n",
    "y_input = Input(shape=(sample[0][1].shape[-1],))\n",
    "y_seq = Dense(2*sample[0][1].shape[-1], activation='relu')(y_input)\n",
    "\n",
    "comb_seq = concatenate([x_seq,y_seq])\n",
    "comb_seq = Dense(128, activation='relu')(comb_seq)\n",
    "comb_seq = Dense(2, activation='softmax')(comb_seq)\n",
    "model = Model([x_input,y_input], comb_seq)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience, best_los = MAX_PATIENCE, 1E10\n",
    "for epoch in range(EPOCHS): # user training loop for dynamic memory allocation\n",
    "\n",
    "    print('Epoch: %d/%d' % (epoch+1,EPOCHS), flush=True)\n",
    "    random.shuffle(s_train)\n",
    "    \n",
    "    train_ret = []\n",
    "    for batch in range(n_training//BATCH_SIZE):\n",
    "    \n",
    "        # prepare taining sample, padding the 2PC inputs into the same shape within the batch\n",
    "        b_train = s_train[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        max_perm = max([len(x) for x,y,z in b_train])\n",
    "        x_train = np.array([np.pad(x,((0,max_perm-len(x)),(0,0)),mode='constant',constant_values=0.) for x,y,z in b_train])\n",
    "        y_train = np.array([y for x,y,z in b_train])\n",
    "        z_train = np.array([z for x,y,z in b_train])\n",
    "        \n",
    "        ret = model.train_on_batch([x_train,y_train], z_train)\n",
    "        train_ret.append(ret)\n",
    "        train_los = np.array(train_ret)[:,0].mean()\n",
    "        train_acc = np.array(train_ret)[:,1].mean()\n",
    "        print('\\rbatch: %05d/%05d, loss: %.4f, acc: %.4f' % (batch+1,n_training//BATCH_SIZE,train_los,train_acc),end='', flush=True)\n",
    "        \n",
    "    test_ret = []\n",
    "    for batch in range(n_testing//BATCH_SIZE):\n",
    "    \n",
    "        # prepare testing sample\n",
    "        b_test = s_test[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        max_perm = max([len(x) for x,y,z in b_test])\n",
    "        x_test = np.array([np.pad(x,((0,max_perm-len(x)),(0,0)),mode='constant',constant_values=0.) for x,y,z in b_test])\n",
    "        y_test = np.array([y for x,y,z in b_test])\n",
    "        z_test = np.array([z for x,y,z in b_test])\n",
    "            \n",
    "        ret = model.test_on_batch([x_test,y_test], z_test)\n",
    "        test_ret.append(ret)\n",
    "        \n",
    "    test_los = np.array(test_ret)[:,0].mean()\n",
    "    test_acc = np.array(test_ret)[:,1].mean()\n",
    "    print(', val_loss: %.4f, val_acc: %.4f' % (test_los,test_acc), flush=True)\n",
    "    \n",
    "    # early stopping by test_los from testing sample\n",
    "    # one has to measure the actual performance from another independent sample!\n",
    "    if test_los<best_los:\n",
    "        best_los = test_los\n",
    "        model.save_weights(WEIGHTS_2PCNN)\n",
    "        patience = MAX_PATIENCE\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience<=0:\n",
    "            print('Stopped; no improvement after %d epochs.' % MAX_PATIENCE, flush=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##=======================================================================loadmodel\n",
    "# read back the weights\n",
    "WEIGHTS_2PCNN=model.load_weights(WEIGHTS_2PCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[i[0] for i in sample]\n",
    "y=[i[2] for i in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_perm = max([len(x) for x,y,z in sample])\n",
    "x_test = np.array([np.pad(x,((0,max_perm-len(x)),(0,0)),mode='constant',constant_values=0.) for x,y,z in sample])\n",
    "y_test = np.array([y for x,y,z in sample])\n",
    "z_test = np.array([z for x,y,z in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score=model.predict([x_test,y_test])\n",
    "test=[i[0] for i in z_test]\n",
    "y_score=[i[1] for i in y_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr , tpr , thresholds = roc_curve ( test , y_score)\n",
    "roc_auc = auc(tpr,fpr )\n",
    "print(\"The area under the curves are:\")\n",
    "print(\"AUC:{0:.9f}\".format(roc_auc))\n",
    "# FalsePositiveFull, TruePositiveFull, ThresholdFull = metrics.roc_curve(y_test,Predictions)\n",
    "plt.figure(figsize=(8,16))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(tpr,fpr, label='Fully supervised: AUC={0:.9f}'.format(roc_auc))\n",
    "plt.ylabel('1-False Positive Rate',fontsize=20)\n",
    "plt.xlabel('True Positive Rate',fontsize=20)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(bbox_to_anchor=(0.8, -0.17),ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./roc.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
